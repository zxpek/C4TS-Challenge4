{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with custom scripts\n",
    "\n",
    "This notebook is meant for the scenario where JMC's data science team might bring their own scripts for training, potentially their own custom libraries. In this case, we will use scikit learn to simulate a custom library that might be used.\n",
    "\n",
    "## Setting up AzureML Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found the config file in: C:\\Users\\zhpek\\Desktop\\C4TS-Challenge4\\aml_config\\config.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./c4ts-customlib\\\\AssetData_Historical.csv'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "from azureml.core import Experiment\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "exp_name = 'c4ts-customlib'\n",
    "exp = Experiment(workspace = ws, name = exp_name)\n",
    "\n",
    "project_folder = './{}'.format(exp_name)\n",
    "os.makedirs(project_folder, exist_ok=True)\n",
    "shutil.copy('AssetData_Historical.csv', project_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Dataprep scripts\n",
    "\n",
    "We make the same steps we do with the AutoML use case. In this case, we make a $\\texttt{prepare}$ function to simplify and clean up the training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./c4ts-customlib/utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./c4ts-customlib/utils.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "def cleanLongLat(l):\n",
    "    split = l.str.split(',', expand=True)\n",
    "    split = (split[0]+'.'+[i if len(i)>1 else i+'0' for i in split[1]]).astype(float)\n",
    "    return(split)\n",
    "    \n",
    "class scaler:\n",
    "    def __init__(self, x = None):\n",
    "        if type(x) == pd.core.frame.DataFrame:\n",
    "            self.fit(x)\n",
    "        elif x == None:\n",
    "            self.x = None\n",
    "            self.mean = None\n",
    "            self.var = None\n",
    "        else:\n",
    "            raise Exception('Require pandas.DF input')\n",
    "\n",
    "\n",
    "    def fit(self, x):\n",
    "        self.x = x\n",
    "        self.mean = x.mean()\n",
    "        self.var = x.var()\n",
    "\n",
    "    def scale(self, new_x):\n",
    "        result = (new_x - self.mean) / np.sqrt(self.var)\n",
    "        return (result)\n",
    "    \n",
    "def prepare(X, fit = False, scaler_obj = None, pca_obj = None):\n",
    "    \n",
    "    if fit:\n",
    "        s = scaler(X)\n",
    "        pca = PCA()\n",
    "        X_ = s.scale(X)\n",
    "        X_ = pca.fit_transform(X_)\n",
    "        X_ = X_[:,:10]\n",
    "        \n",
    "        return(X_, s, pca)\n",
    "    else:\n",
    "        if scaler_obj == None or pca_obj == None:\n",
    "            raise Exception('Non fitting requires scaler/pca obj')\n",
    "        X_ = pca_obj.transform(scaler_obj.scale(X))[:,:10]\n",
    "        return X_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Script\n",
    "\n",
    "We use scikit learn's GB Classifier as a quick example with grid search using 5-fold CV over the number of trees and learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./c4ts-customlib/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./c4ts-customlib/train.py\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from azureml.core.run import Run\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from utils import *\n",
    "    \n",
    "os.makedirs('./outputs', exist_ok=True)\n",
    "\n",
    "# Data Preparation\n",
    "df = pd.read_csv('AssetData_Historical.csv')\n",
    "df.drop(['Machine_ID', 'District'], axis=1, inplace=True)\n",
    "df['Latitude'] = cleanLongLat(df['Latitude'])\n",
    "df['Longitude'] = cleanLongLat(df['Longitude'])\n",
    "X = df.drop('Failure_NextHour', 1)\n",
    "y = df['Failure_NextHour']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, stratify = y)\n",
    "\n",
    "X_prep, s, pca= prepare(X_train, fit = True)\n",
    "\n",
    "run = Run.get_context()\n",
    "\n",
    "param_grid = {'learning_rate': [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3],\n",
    "              'n_estimators': [100, 200, 300, 400, 500]\n",
    "}\n",
    "\n",
    "model = GradientBoostingClassifier(loss = 'exponential')\n",
    "kf = StratifiedKFold(n_splits = 5, shuffle = True)\n",
    "gridsearch = GridSearchCV(model, param_grid, \n",
    "                          scoring = 'f1_weighted',\n",
    "                          n_jobs = -1,\n",
    "                          cv = kf)\n",
    "weights = y_train * 3 + 1\n",
    "result = gridsearch.fit(X_prep, y_train, sample_weight = weights)\n",
    "\n",
    "run.log('bestScore', result.best_score_)\n",
    "run.log('bestParam', result.best_params_)\n",
    "run.log('valMean', result.cv_results_['mean_test_score'])\n",
    "run.log('valStd', result.cv_results_['std_test_score'])\n",
    "run.log('valParams', result.cv_results_['params'])\n",
    "run.log('FeatureImportance', result.feature_importances_)\n",
    "\n",
    "#################\n",
    "#Fit Final Model#\n",
    "#################\n",
    "\n",
    "X, s, pca = prepare(X, fit = True)\n",
    "best_model = result.estimator.fit(X, y, sample_weight = y*4 + 1)\n",
    "\n",
    "pickle.dump(s, open('./outputs/scaler.pkl', 'wb'))\n",
    "pickle.dump(pca,open('./outputs/pca_transform.pkl','wb'))\n",
    "\n",
    "import time\n",
    "model_name = 'GBT_{}'.format(time.time())\n",
    "with open(model_name, 'wb') as f:\n",
    "    joblib.dump(value = best_model, filename = './outputs/{}.pkl'.format(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Experiment</th><th>Id</th><th>Type</th><th>Status</th><th>Details Page</th><th>Docs Page</th></tr><tr><td>c4ts-customlib</td><td>c4ts-customlib_1548125115512</td><td>azureml.scriptrun</td><td>Queued</td><td><a href=\"https://mlworkspace.azure.ai/portal/subscriptions/50fb2758-5add-47ee-b8f2-9c9ae596fed5/resourceGroups/pekamlws-sea/providers/Microsoft.MachineLearningServices/workspaces/pekamlws/experiments/c4ts-customlib/runs/c4ts-customlib_1548125115512\" target=\"_blank\" rel=\"noopener\">Link to Azure Portal</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.script_run.ScriptRun?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>"
      ],
      "text/plain": [
       "Run(Experiment: c4ts-customlib,\n",
       "Id: c4ts-customlib_1548125115512,\n",
       "Type: azureml.scriptrun,\n",
       "Status: Queued)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.compute import ComputeTarget#, AmlCompute\n",
    "\n",
    "cpu_cluster = ComputeTarget(workspace=ws, name= \"pekamlcompute\")\n",
    "run_config = RunConfiguration(framework=\"python\")\n",
    "run_config.target = cpu_cluster.name\n",
    "run_config.environment.docker.enabled = True\n",
    "run_config.environment.python.conda_dependencies = CondaDependencies.create(conda_packages=['scikit-learn', 'pandas','numpy'])\n",
    "\n",
    "from azureml.core import Run\n",
    "from azureml.core import ScriptRunConfig\n",
    "\n",
    "src = ScriptRunConfig(source_directory=project_folder, \n",
    "                      script='train.py', \n",
    "                      run_config=run_config) \n",
    "run = exp.submit(config=src)\n",
    "run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Shows output of the run on stdout.\n",
    "run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run.get_metrics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
