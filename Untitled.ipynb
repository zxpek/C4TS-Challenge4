{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "def cleanLongLat(l):\n",
    "    split = l.str.split(',', expand=True)\n",
    "    split = (split[0]+'.'+[i if len(i)>1 else i+'0' for i in split[1]]).astype(float)\n",
    "    return(split)\n",
    "    \n",
    "class scaler:\n",
    "    def __init__(self, x = None):\n",
    "        if type(x) == pd.core.frame.DataFrame:\n",
    "            self.fit(x)\n",
    "        elif x == None:\n",
    "            self.x = None\n",
    "            self.mean = None\n",
    "            self.var = None\n",
    "        else:\n",
    "            raise Exception('Require pandas.DF input')\n",
    "\n",
    "\n",
    "    def fit(self, x):\n",
    "        self.x = x\n",
    "        self.mean = x.mean()\n",
    "        self.var = x.var()\n",
    "\n",
    "    def scale(self, new_x):\n",
    "        result = (new_x - self.mean) / np.sqrt(self.var)\n",
    "        return (result)\n",
    "    \n",
    "def prepare(X, fit = False, scaler_obj = None, pca_obj = None):\n",
    "    \n",
    "    if fit:\n",
    "        s = scaler(X)\n",
    "        pca = PCA()\n",
    "        X_ = s.scale(X)\n",
    "        X_ = pca.fit_transform(X_)\n",
    "        X_ = X_[:,:10]\n",
    "        \n",
    "        return(X_, s, pca)\n",
    "    else:\n",
    "        if scaler_obj == None or pca_obj == None:\n",
    "            raise Exception('Non fitting requires scaler/pca obj')\n",
    "        X_ = pca_obj.transform(scaler_obj.scale(X))[:,:10]\n",
    "        return X_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from azureml.core.run import Run\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from utils import *\n",
    "    \n",
    "os.makedirs('./outputs', exist_ok=True)\n",
    "\n",
    "# Data Preparation\n",
    "df = pd.read_csv('AssetData_Historical.csv')\n",
    "df.drop(['Machine_ID', 'District'], axis=1, inplace=True)\n",
    "df['Latitude'] = cleanLongLat(df['Latitude'])\n",
    "df['Longitude'] = cleanLongLat(df['Longitude'])\n",
    "X = df.drop('Failure_NextHour', 1)\n",
    "y = df['Failure_NextHour']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, stratify = y)\n",
    "\n",
    "X_prep, s, pca= prepare(X_train, fit = True)\n",
    "\n",
    "#run = Run.get_context()\n",
    "\n",
    "data = {\"train\": {\"X\": X_train, \"y\": y_train},\n",
    "        \"test\": {\"X\": X_test, \"y\": y_test}}\n",
    "\n",
    "param_grid = {'learning_rate': [0.0001, 0.001],# 0.01, 0.1, 0.2, 0.3],\n",
    "              'n_estimators': [100]#, 200, 300, 400, 500]\n",
    "}\n",
    "\n",
    "model = GradientBoostingClassifier(loss = 'exponential')\n",
    "kf = StratifiedKFold(n_splits = 5, shuffle = True)\n",
    "gridsearch = GridSearchCV(model, param_grid, \n",
    "                          scoring = 'f1_weighted',\n",
    "                          n_jobs = -1,\n",
    "                          cv = kf)\n",
    "weights = y_train * 3 + 1\n",
    "result = gridsearch.fit(X_prep, y_train, sample_weight = weights)\n",
    "\n",
    "#run.log('bestScore', result.best_score_)\n",
    "#run.log('bestParam', result.best_params_)\n",
    "#run.log('valMean', result.cv_results_['mean_test_score'])\n",
    "#run.log('valStd', result.cv_results_['std_test_score'])\n",
    "#run.log('valParams', result.cv_results_['params'])\n",
    "#run.log('FeatureImportance', result.)\n",
    "\n",
    "#################\n",
    "#Fit Final Model#\n",
    "#################\n",
    "\n",
    "X, s, pca = prepare(X, fit = True)\n",
    "best_model = result.estimator.fit(X, y, sample_weight = y*4 + 1)\n",
    "\n",
    "pickle.dump(s, open('./outputs/scaler.pkl', 'wb'))\n",
    "pickle.dump(pca,open('./outputs/pca_transform.pkl','wb'))\n",
    "\n",
    "import time\n",
    "model_name = 'GBT_{}'.format(time.time())\n",
    "with open(model_name, 'wb') as f:\n",
    "    joblib.dump(value = best_model, filename = './outputs/{}.pkl'.format(model_name))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
